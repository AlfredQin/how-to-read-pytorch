{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to hide it.\n",
    "# It defines plot_progress() for plotting an optimization trace.\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_progress(bowl, track, losses):\n",
    "    # Draw the contours of the objective function, and x, and y\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 5))\n",
    "    for size in torch.linspace(0.1, 1.0, 10):\n",
    "        angle = torch.linspace(0, 6.3, 100)\n",
    "        circle = torch.stack([angle.sin(), angle.cos()])\n",
    "        ellipse = torch.mm(torch.inverse(skew), circle) * size\n",
    "        ax1.plot(ellipse[0,:], ellipse[1,:], color='skyblue')\n",
    "    track = torch.stack(track).t()\n",
    "    ax1.set_title('progress of x')\n",
    "    ax1.plot(track[0,:], track[1,:], marker='o')\n",
    "    ax1.set_ylim(-1, 1)\n",
    "    ax1.set_xlim(-1.6, 1.6)\n",
    "    ax1.set_ylabel('x[1]')\n",
    "    ax1.set_xlabel('x[0]')\n",
    "    ax2.set_title('progress of y')\n",
    "    ax2.xaxis.set_major_locator(matplotlib.ticker.MaxNLocator(integer=True))\n",
    "    ax2.plot(range(len(losses)), losses, marker='o')\n",
    "    ax2.set_ylabel('objective')\n",
    "    ax2.set_xlabel('iteration')\n",
    "    fig.show()\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>function toggle_code(){$('.rendered.selected div.input').toggle().find('textarea').focus();} $(toggle_code)</script>\n",
    "<a href=\"javascript:toggle_code()\">Toggle</a> the code for plot_progress.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Optimizers\n",
    "==================\n",
    "\n",
    "Pytorch includes several optimization algorithms.\n",
    "\n",
    "Optimizers have a simple job: given gradients of an objective with respect to a set of input parameters, adjust the parameters reduce the objective.  They do this by modifying each parameter by a small amount in the direction given by the gradient.\n",
    "\n",
    "The actual optmization algorithms employ a number of techniques to make the process faster and more robust as repeated steps are taken, by trying to adapt to the shape of the objective surface as it is explored.  The simplest method is SGD-with-momentum, which is implemented in pytorch as `pytorch.optim.SGD`.\n",
    "\n",
    "Using SGD\n",
    "---------\n",
    "\n",
    "To use SGD, you need to calculate your objective and fill in gradients on all the parameters before it can take a step.\n",
    "\n",
    "  1. Set your parameters (x in this case) to `x.requires_grad = True` so autograd tracks them (line 5).\n",
    "  2. Create the optimizer and tell it about the parameters to adjust (`[x]` here) (line 6).\n",
    "  3. In a loop, compute your objective, then call `objective.backward()` to fill in `x.grad` and then `optimizer.step()` to adjust `x` accordingly (lines 12-15).\n",
    "  \n",
    "**Remember to zero gradient.** Notice that we use `optimizer.zero_grad()` each time to set x.grad to zero before recomputing gradients; if we do not do this, then the new gradient will be added to the old one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_init = torch.randn(2)\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.SGD([x], lr=0.1, momentum=0.5)\n",
    "\n",
    "bowl = torch.tensor([[ 0.4410, -1.0317], [-0.2844, -0.1035]])\n",
    "track, losses = [], []\n",
    "\n",
    "for iter in range(21):\n",
    "    objective = torch.mm(bowl, x[:,None]).norm()\n",
    "    optimizer.zero_grad()\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "    track.append(x.detach().clone())\n",
    "    losses.append(objective.detach())\n",
    "    \n",
    "plot_progress(bowl, track, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using other optimizers\n",
    "----------------------\n",
    "\n",
    "Other optimizers are similar.  Adam is a popular adaptive method that does well without much tuning, and can be dropped in to replace plain SGD.\n",
    "\n",
    "Some other fancy optimizers, such as LBFGS, need to be given an objective function that they can call repeatedly to probe gradients themselves.  Examples can be found elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below uses Adam\n",
    "x = x_init.clone()\n",
    "x.requires_grad = True\n",
    "optimizer = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "track, losses = [], []\n",
    "\n",
    "for iter in range(21):\n",
    "    objective = torch.mm(bowl, x[:,None]).norm()\n",
    "    optimizer.zero_grad()\n",
    "    objective.backward()\n",
    "    optimizer.step()\n",
    "    track.append(x.detach().clone())\n",
    "    losses.append(objective.detach())\n",
    "    \n",
    "plot_progress(bowl, track, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting learning rate schedules\n",
    "---------------------------------\n",
    "\n",
    "One of the easiest ways to improve convergence is to reduce the learning rate as more iterations are done.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}